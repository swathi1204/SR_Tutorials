{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.GridFSBucketReadStream = void 0;\nconst stream_1 = require(\"stream\");\nconst error_1 = require(\"../error\");\n/**\n * A readable stream that enables you to read buffers from GridFS.\n *\n * Do not instantiate this class directly. Use `openDownloadStream()` instead.\n * @public\n */\nclass GridFSBucketReadStream extends stream_1.Readable {\n  /**\n   * @param chunks - Handle for chunks collection\n   * @param files - Handle for files collection\n   * @param readPreference - The read preference to use\n   * @param filter - The filter to use to find the file document\n   * @internal\n   */\n  constructor(chunks, files, readPreference, filter, options) {\n    super();\n    this.s = {\n      bytesToTrim: 0,\n      bytesToSkip: 0,\n      bytesRead: 0,\n      chunks,\n      expected: 0,\n      files,\n      filter,\n      init: false,\n      expectedEnd: 0,\n      options: {\n        start: 0,\n        end: 0,\n        ...options\n      },\n      readPreference\n    };\n  }\n  /**\n   * Reads from the cursor and pushes to the stream.\n   * Private Impl, do not call directly\n   * @internal\n   */\n  _read() {\n    if (this.destroyed) return;\n    waitForFile(this, () => doRead(this));\n  }\n  /**\n   * Sets the 0-based offset in bytes to start streaming from. Throws\n   * an error if this stream has entered flowing mode\n   * (e.g. if you've already called `on('data')`)\n   *\n   * @param start - 0-based offset in bytes to start streaming from\n   */\n  start() {\n    let start = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n    throwIfInitialized(this);\n    this.s.options.start = start;\n    return this;\n  }\n  /**\n   * Sets the 0-based offset in bytes to start streaming from. Throws\n   * an error if this stream has entered flowing mode\n   * (e.g. if you've already called `on('data')`)\n   *\n   * @param end - Offset in bytes to stop reading at\n   */\n  end() {\n    let end = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n    throwIfInitialized(this);\n    this.s.options.end = end;\n    return this;\n  }\n  /**\n   * Marks this stream as aborted (will never push another `data` event)\n   * and kills the underlying cursor. Will emit the 'end' event, and then\n   * the 'close' event once the cursor is successfully killed.\n   *\n   * @param callback - called when the cursor is successfully closed or an error occurred.\n   */\n  abort(callback) {\n    this.push(null);\n    this.destroyed = true;\n    if (this.s.cursor) {\n      this.s.cursor.close(error => {\n        this.emit(GridFSBucketReadStream.CLOSE);\n        callback && callback(error);\n      });\n    } else {\n      if (!this.s.init) {\n        // If not initialized, fire close event because we will never\n        // get a cursor\n        this.emit(GridFSBucketReadStream.CLOSE);\n      }\n      callback && callback();\n    }\n  }\n}\nexports.GridFSBucketReadStream = GridFSBucketReadStream;\n/**\n * An error occurred\n * @event\n */\nGridFSBucketReadStream.ERROR = 'error';\n/**\n * Fires when the stream loaded the file document corresponding to the provided id.\n * @event\n */\nGridFSBucketReadStream.FILE = 'file';\n/**\n * Emitted when a chunk of data is available to be consumed.\n * @event\n */\nGridFSBucketReadStream.DATA = 'data';\n/**\n * Fired when the stream is exhausted (no more data events).\n * @event\n */\nGridFSBucketReadStream.END = 'end';\n/**\n * Fired when the stream is exhausted and the underlying cursor is killed\n * @event\n */\nGridFSBucketReadStream.CLOSE = 'close';\nfunction throwIfInitialized(stream) {\n  if (stream.s.init) {\n    throw new error_1.MongoGridFSStreamError('Options cannot be changed after the stream is initialized');\n  }\n}\nfunction doRead(stream) {\n  if (stream.destroyed) return;\n  if (!stream.s.cursor) return;\n  if (!stream.s.file) return;\n  stream.s.cursor.next((error, doc) => {\n    if (stream.destroyed) {\n      return;\n    }\n    if (error) {\n      stream.emit(GridFSBucketReadStream.ERROR, error);\n      return;\n    }\n    if (!doc) {\n      stream.push(null);\n      if (!stream.s.cursor) return;\n      stream.s.cursor.close(error => {\n        if (error) {\n          stream.emit(GridFSBucketReadStream.ERROR, error);\n          return;\n        }\n        stream.emit(GridFSBucketReadStream.CLOSE);\n      });\n      return;\n    }\n    if (!stream.s.file) return;\n    const bytesRemaining = stream.s.file.length - stream.s.bytesRead;\n    const expectedN = stream.s.expected++;\n    const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);\n    if (doc.n > expectedN) {\n      return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n    }\n    if (doc.n < expectedN) {\n      return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n    }\n    let buf = Buffer.isBuffer(doc.data) ? doc.data : doc.data.buffer;\n    if (buf.byteLength !== expectedLength) {\n      if (bytesRemaining <= 0) {\n        return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected file length ${stream.s.file.length} bytes but already read ${stream.s.bytesRead} bytes`));\n      }\n      return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`));\n    }\n    stream.s.bytesRead += buf.byteLength;\n    if (buf.byteLength === 0) {\n      return stream.push(null);\n    }\n    let sliceStart = null;\n    let sliceEnd = null;\n    if (stream.s.bytesToSkip != null) {\n      sliceStart = stream.s.bytesToSkip;\n      stream.s.bytesToSkip = 0;\n    }\n    const atEndOfStream = expectedN === stream.s.expectedEnd - 1;\n    const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;\n    if (atEndOfStream && stream.s.bytesToTrim != null) {\n      sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;\n    } else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {\n      sliceEnd = bytesLeftToRead;\n    }\n    if (sliceStart != null || sliceEnd != null) {\n      buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);\n    }\n    stream.push(buf);\n    return;\n  });\n}\nfunction init(stream) {\n  const findOneOptions = {};\n  if (stream.s.readPreference) {\n    findOneOptions.readPreference = stream.s.readPreference;\n  }\n  if (stream.s.options && stream.s.options.sort) {\n    findOneOptions.sort = stream.s.options.sort;\n  }\n  if (stream.s.options && stream.s.options.skip) {\n    findOneOptions.skip = stream.s.options.skip;\n  }\n  stream.s.files.findOne(stream.s.filter, findOneOptions, (error, doc) => {\n    if (error) {\n      return stream.emit(GridFSBucketReadStream.ERROR, error);\n    }\n    if (!doc) {\n      const identifier = stream.s.filter._id ? stream.s.filter._id.toString() : stream.s.filter.filename;\n      const errmsg = `FileNotFound: file ${identifier} was not found`;\n      // TODO(NODE-3483)\n      const err = new error_1.MongoRuntimeError(errmsg);\n      err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor\n      return stream.emit(GridFSBucketReadStream.ERROR, err);\n    }\n    // If document is empty, kill the stream immediately and don't\n    // execute any reads\n    if (doc.length <= 0) {\n      stream.push(null);\n      return;\n    }\n    if (stream.destroyed) {\n      // If user destroys the stream before we have a cursor, wait\n      // until the query is done to say we're 'closed' because we can't\n      // cancel a query.\n      stream.emit(GridFSBucketReadStream.CLOSE);\n      return;\n    }\n    try {\n      stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);\n    } catch (error) {\n      return stream.emit(GridFSBucketReadStream.ERROR, error);\n    }\n    const filter = {\n      files_id: doc._id\n    };\n    // Currently (MongoDB 3.4.4) skip function does not support the index,\n    // it needs to retrieve all the documents first and then skip them. (CS-25811)\n    // As work around we use $gte on the \"n\" field.\n    if (stream.s.options && stream.s.options.start != null) {\n      const skip = Math.floor(stream.s.options.start / doc.chunkSize);\n      if (skip > 0) {\n        filter['n'] = {\n          $gte: skip\n        };\n      }\n    }\n    stream.s.cursor = stream.s.chunks.find(filter).sort({\n      n: 1\n    });\n    if (stream.s.readPreference) {\n      stream.s.cursor.withReadPreference(stream.s.readPreference);\n    }\n    stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);\n    stream.s.file = doc;\n    try {\n      stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);\n    } catch (error) {\n      return stream.emit(GridFSBucketReadStream.ERROR, error);\n    }\n    stream.emit(GridFSBucketReadStream.FILE, doc);\n    return;\n  });\n}\nfunction waitForFile(stream, callback) {\n  if (stream.s.file) {\n    return callback();\n  }\n  if (!stream.s.init) {\n    init(stream);\n    stream.s.init = true;\n  }\n  stream.once('file', () => {\n    callback();\n  });\n}\nfunction handleStartOption(stream, doc, options) {\n  if (options && options.start != null) {\n    if (options.start > doc.length) {\n      throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be more than the length of the file (${doc.length})`);\n    }\n    if (options.start < 0) {\n      throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);\n    }\n    if (options.end != null && options.end < options.start) {\n      throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be greater than stream end (${options.end})`);\n    }\n    stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;\n    stream.s.expected = Math.floor(options.start / doc.chunkSize);\n    return options.start - stream.s.bytesRead;\n  }\n  throw new error_1.MongoInvalidArgumentError('Start option must be defined');\n}\nfunction handleEndOption(stream, doc, cursor, options) {\n  if (options && options.end != null) {\n    if (options.end > doc.length) {\n      throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be more than the length of the file (${doc.length})`);\n    }\n    if (options.start == null || options.start < 0) {\n      throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);\n    }\n    const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;\n    cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);\n    stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);\n    return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;\n  }\n  throw new error_1.MongoInvalidArgumentError('End option must be defined');\n}\n//# sourceMappingURL=download.js.map","map":null,"metadata":{},"sourceType":"script","externalDependencies":[]}